def text_processing_class_kylie():
    string = [
    "from sklearn.base import TransformerMixin",
    "import re",
    "import spacy",
    "nlp = spacy.load('en')",
    "from spacy import en\n",
    "class ProcessText(TransformerMixin):",
    '    """This class will apply the following functions.',
    "    - Remove carats and text between them.",
    "    - Join names together with spaCy entity recognizer.",
    "    - Remove stopwords.",
    "    - Lemmatize the words.",
    '    - Remove punctuation."""\n',
    "    def transform(self, X, **transform_params):",
    "        names = clean_text_names(X)",
    "        corpus_adjusted_names = clean_characters_re(names)",
    "        names_joined = clean_text(corpus_adjusted_names)",
    "        return second_round_of_lemmatization(names_joined)\n",
    "    def fit(self, X, y=None, **fit_params):",
    "        return self\n",
    "    def get_params(self, deep=True):",
    "        return {}\n",
    "# THE BELOW FUNCTIONS ARE USED IN THE ABOVE CLASS.\n",
    "def join_names(parsed_sentence):",
    "    match = 0",
    "    holder = 0",
    "    sentence = []",
    "    for token in parsed_sentence:",
    "        if token.ent_type_ == u'PERSON':",
    "            if match == 1:",
    "                name = u'_'.join([holder, token.orth_])",
    "                sentence.append(name)",
    "                match = 0",
    "            else:",
    "                match = 1",
    "                holder = token.orth_",
    "        elif match == 1:",
    "            sentence.append(holder)",
    "            sentence.append(token.orth_)",
    "            holder = 0",
    "            match = 0",
    "        else:",
    "            sentence.append(token.orth_)",
    "    return sentence\n",
    "def punct_space(token):",
    "    return token.is_punct or token.is_space\n",
    "def clean_text(corpus):",
    "    holder = []",
    "    for doc in nlp.pipe(corpus, batch_size=1000, n_threads=4):",
    "        # lemmatize the text, removing punctuation and whitespace",
    "        lem_doc = [token.lemma_ for token in doc if not punct_space(token)]",
    "        # remove any remaining stopwords",
    "        stop_doc = [term for term in lem_doc if term not in spacy.en.STOPWORDS]",
    "        # write the transformed review as a line in the new file",
    "        clean_doc = u' '.join(stop_doc)",
    "        holder.append(clean_doc)",
    "    return holder\n",
    "def second_round_of_lemmatization(corpus):",
    "    holder = []",
    "    for doc in nlp.pipe(corpus, batch_size=1000, n_threads=4):",
    "        # lemmatize the text, removing punctuation and whitespace",
    "        lem_doc = [token.lemma_ for token in doc]",
    "        # write the transformed review as a line in the new file",
    "        holder.append(u' '.join(lem_doc)",
    "    return holder\n",
    "def clean_characters_re(corpus): # remove specific characters from the corpus.",
    "    holder = []",
    "    for i in corpus:",
    "        text = i.replace('\n', ' ')",
    "        text = re.sub(r'\<.*?\>', '', text)",
    "        text = text.replace('>', ' ')",
    '        holder.append(text.replace("\'", ""))',
    "    return holder\n",
    "def clean_text_names(corpus):",
    "    holder = []",
    "    for doc in nlp.pipe(corpus, batch_size=1000, n_threads=4):",
    "        joined = join_names(doc) # join names together.",
    "        stop_doc = [term for term in joined if term not in spacy.en.STOPWORDS] # remove any remaining stopwords",
    "        holder.append(u' '.join(stop_doc))",
    "    return holder\n\n",
    "%%time",
    "# Instantiate the processing text class.",
    "process = ProcessText()",
    "# fit and transform the class to the raw corpus.",
    "cleaned_text = process.fit_transform(corpus_raw)",
    ]
    from pygments import highlight
    from pygments.formatters import Terminal256Formatter  # Or TerminalFormatter
    from pygments.lexers import PythonLexer
    # Use Pygments to do syntax highlighting
    lexer = PythonLexer()
    formatter = Terminal256Formatter()
    output = highlight(u'\n'.join(string), lexer, formatter)
    print(output)

def custom_text_classifier():
    string = [
    "import numpy as np",
    "from sklearn.base import BaseEstimator, ClassifierMixin",
    "from sklearn.utils.validation import check_is_fitted",
    "from sklearn.linear_model import SGDClassifier",
    "from sklearn.pipeline import make_pipeline",
    "from sklearn.feature_extraction.text import TfidfVectorizer",
    "from sklearn.cross_validation import cross_val_score\n",
    "class PipelineClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self):",
    "        self.vec = TfidfVectorizer()",
    "        # because we are dealing with a multi_class text classification problem we'll use SGDclassifier.",
    "        self.clf = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=10, random_state=42)",
    "        self.pipe = make_pipeline(self.vec, self.clf)\n",
    "    def fit(self, X, y):",
    "        self.X_ = X",
    "        self.y_ = y",
    "        self.pipe.fit(X, y) #fit the model.",
    "        return self.pipe\n",
    "    def predict(self, X):",
    "        check_is_fitted(self, ['X_', 'y_']) # check if fit has been called",
    "        predicted = self.pipe.predict(X)",
    "        return predicted\n",
    "    def score(self, X, y=None):",
    "        return cross_val_score(self.pipe, X, y, cv=3, scoring='accuracy').mean() # runs a cross_val_score on the data.\n\n",
    "%%time",
    "piping_test = PipelineClassifier()      # Instantiate the pipeline classifier.",
    "piping_test.fit(train, labelsTrain)     # Call the fit method on the train and target data.",
    "preds = piping_test.predict(test)       # After fitting, the predict method can be used on the test data.",
    "piping_test.score(train, labelsTrain)   # Calling the score method with passing it the train and target data will return a cross_val score.",
    ]
    from pygments import highlight
    from pygments.formatters import Terminal256Formatter  # Or TerminalFormatter
    from pygments.lexers import PythonLexer
    # Use Pygments to do syntax highlighting
    lexer = PythonLexer()
    formatter = Terminal256Formatter()
    output = highlight(u'\n'.join(string), lexer, formatter)
    print(output)
